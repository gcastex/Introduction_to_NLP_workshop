{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kernel & front-end diagram](Figures/FrontendKernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gcastex/Introduction_to_NLP_workshop/master/Figures/hobbit_banner.png?token=AEOBFLFNHZUJQ46YJYZL26S642OEI\" alt=\"Hobbit banner\"  width=1000/>\n",
    "<img src=\"Figures/hobbit_banner.png\" alt=\"Hobbit banner\"  width=1000/>\n",
    "\n",
    "<span style=\"color:blue;\">In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat: it was a hobbit-hole, and that means comfort.</span>\n",
    "\n",
    "![banner](Figures/hobbit_banner.png)\n",
    "\n",
    "![banner](https://raw.githubusercontent.com/gcastex/Introduction_to_NLP_workshop/master/Figures/hobbit_banner.png?token=AEOBFLFNHZUJQ46YJYZL26S642OEI)\n",
    "\n",
    "# Introduction\n",
    "This notebook presents some common NLP preprocessing methods. I use quotes from _the Hobbit_ as examples.\n",
    "\n",
    "For most cases, I provide the code for both the NLTK and Spacy libraries, and highlight some of the differences that can be observed in the default behaviours of each library. \n",
    "How to customize these behaviours by using the parameters of the libraries is beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download models and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NLTK\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence tokenization\n",
    "\n",
    "**Sentence tokenization**: Split a text into individual sentences.\n",
    "Let's take the text below as an example and use the standard sentence tokenizers in both NLTK and Spacy. NLTK returns 2 sentences, while Spacy returns 3: \n",
    "\n",
    "By default, Spacy considers the **':'** symbol to separate two sentences, while NLTK groups the sequence in a single sentence.\n",
    "\n",
    "Let's use this text as an example: \n",
    "\n",
    "**_<span style=\"color:blue;\">In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat: it was a hobbit-hole, and that means comfort.</span>_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat: it was a hobbit-hole, and that means comfort.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['In a hole in the ground there lived a hobbit.', 'Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat: it was a hobbit-hole, and that means comfort.']\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['In a hole in the ground there lived a hobbit.', 'Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat', ': it was a hobbit-hole, and that means comfort.']\n"
     ]
    }
   ],
   "source": [
    "# Spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(text)\n",
    "\n",
    "sentences = [str(s) for s in doc.sents]\n",
    "print(len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word tokenization\n",
    "**Word tokenization**: Split a text into individual words. We can also tag each word with their corresponding **Part Of Speech** (Verb, adverb, noun, etc).\n",
    "\n",
    "Text example: **_<span style=\"color:blue;\">In a hole in the ground there lived a hobbit.</span>_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In a hole in the ground there lived a hobbit.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'a', 'hole', 'in', 'the', 'ground', 'there', 'lived', 'a', 'hobbit', '.']\n",
      "[('In', 'IN'), ('a', 'DT'), ('hole', 'NN'), ('in', 'IN'), ('the', 'DT'), ('ground', 'NN'), ('there', 'RB'), ('lived', 'VBD'), ('a', 'DT'), ('hobbit', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(word_tokens)\n",
    "\n",
    "# With Part of Speech tagging\n",
    "tagged = nltk.pos_tag(word_tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In ADP\n",
      "a DET\n",
      "hole NOUN\n",
      "in ADP\n",
      "the DET\n",
      "ground NOUN\n",
      "there ADV\n",
      "lived VERB\n",
      "a DET\n",
      "hobbit NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Spacy\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print words with their POS\n",
    "for word in doc:\n",
    "    print(word.text,  word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic word tokenization can be performed by splitting a string on white spaces with the `.split()`method.\n",
    "However, it doesn't take into account words that are glued together like the negation in:\n",
    "\n",
    "**_<span style=\"color:blue;\">Aren't you the burglar?</span>_**\n",
    "\n",
    "However, both libraries take advantage of their knowledge of the english language to separate **_are_** and **_n't_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', \"n't\", 'you', 'the', 'burglar', '?']\n",
      "['Are', \"n't\", 'you', 'the', 'burglar', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Aren't you the burglar?\"\n",
    "\n",
    "# NLTK\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(word_tokens)\n",
    "\n",
    "# Spacy\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(text)\n",
    "print([word.text for word in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "N-grams are groups of N consecutive words appearing in a text.\n",
    "Let's code a function to extract all the N-grams of length _N_ from a text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example using Spacy\n",
    "\n",
    "def get_N_Grams(text, n):\n",
    "    ngrams = []\n",
    "    nlp = en_core_web_sm.load()\n",
    "    word_list = nlp(text)\n",
    "    word_list = [word.text for word in word_list if word.text.isalnum()]\n",
    "    for i in range(len(word_list)-(n-1)):\n",
    "        ngrams.append(word_list[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract all the 3-grams (also called trigrams) from the text below.\n",
    "\n",
    "Text example: **_<p style=\"color:blue;\">In a hole in the ground there lived a hobbit.</p>_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['In', 'a', 'hole'], ['a', 'hole', 'in'], ['hole', 'in', 'the'], ['in', 'the', 'ground'], ['the', 'ground', 'there'], ['ground', 'there', 'lived'], ['there', 'lived', 'a'], ['lived', 'a', 'hobbit']]\n"
     ]
    }
   ],
   "source": [
    "text = \"In a hole in the ground there lived a hobbit.\"\n",
    "print(get_N_Grams(text,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "Both are methods to get the root of a word. \n",
    "* Stemming: The root doesnâ€™t need to be an existing word.\n",
    "* Lemmatization: The root must exist in the vocabulary.\n",
    "\n",
    "## Stemming\n",
    "Note: There's no stemming in Spacy.\n",
    "\n",
    "Text example: **_<span style=\"color:blue;\">We don't want any adventures here, thank you!</span>_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"We don't want any adventures here, thank you!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'do', \"n't\", 'want', 'ani', 'adventur', 'here', ',', 'thank', 'you', '!']\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "tokenized = nltk.word_tokenize(text)\n",
    "stemmed_text = [porter.stem(word) for word in tokenized]\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Notice that all the word roots obtained after lemmatization can actually be found in a dictionary.\n",
    "\n",
    "Text example: **_<span style=\"color:blue;\">Elvish singing is not a thing to miss</span>_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Elvish singing is not a thing to miss\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elvish Elvish\n",
      "singing sing\n",
      "is be\n",
      "not not\n",
      "a a\n",
      "thing thing\n",
      "to to\n",
      "miss miss\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnetLem = WordNetLemmatizer()\n",
    "tokenized = nltk.word_tokenize(text)\n",
    "for word in tokenized:\n",
    "    print(word, wordnetLem.lemmatize(word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elvish elvish\n",
      "singing singing\n",
      "is be\n",
      "not not\n",
      "a a\n",
      "thing thing\n",
      "to to\n",
      "miss miss\n"
     ]
    }
   ],
   "source": [
    "# Spacy\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(text)\n",
    "\n",
    "for word in doc:\n",
    "    print(word.text,  word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the word _singing_ , NLTK takes the root _sing_ , while Spacy keeps the word _singing_ as is. This is due to the fact that in this sentence, _singing_ is a noun. When faced with verbs ending in _-ing_ , both libraries use the base form of the verb, as can be seen in the other example below.\n",
    "\n",
    "Text example: **_<p style=\"color:blue;\">This is the story of how a Baggins had an adventure, and found himself doing and saying things altogether unexpected.</p>_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this be the story of how a Baggins have an adventure , and find -PRON- do and say thing altogether unexpected . \n"
     ]
    }
   ],
   "source": [
    "text = \"This is the story of how a Baggins had an adventure, and found himself doing and saying things altogether unexpected.\"\n",
    "doc = nlp(text)\n",
    "s=\"\"\n",
    "for word in doc:\n",
    "    s+=word.lemma_+\" \"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This be the story of how a Baggins have an adventure , and find himself do and say things altogether unexpected . \n"
     ]
    }
   ],
   "source": [
    "text = \"This is the story of how a Baggins had an adventure, and found himself doing and saying things altogether unexpected.\"\n",
    "tokenized = nltk.word_tokenize(text)\n",
    "s=\"\"\n",
    "for word in tokenized:\n",
    "    s+=wordnetLem.lemmatize(word, pos=\"v\")+\" \"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "Stop words are common words that do not add information to the analysis. They are often removed from the text for word based approaches.\n",
    "\n",
    "Let's remove the stop words from the text below.\n",
    "\n",
    "Text example: **_<span style=\"color:blue;\">In a hole in the ground there lived a hobbit.</span>_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In a hole in the ground there lived a hobbit.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hole', 'ground', 'lived', 'hobbit']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english')) \n",
    "\n",
    "tokenized = nltk.word_tokenize(text)\n",
    "filtered = [w.lower() for w in tokenized if (not w.lower() in stop and w.isalnum())]\n",
    "print(filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hole', 'ground', 'lived', 'hobbit']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "doc = nlp(text)\n",
    "token_list = [token.text for token in doc if token.is_punct == False]\n",
    "\n",
    "filtered = [word for word in token_list if nlp.vocab[word].is_stop == False]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "Bag of words is a representation in which each document of a corpus is represented by a vector. The length of the vector is the length of the vocabulary of all the words appearing in the corpus. Each dimension of the vector corresponds to a word in the vocabulary. The vector representation of a document contains a 1 at the position of a word if the document contains said word, and a 0 otherwise.\n",
    "\n",
    "Alternatively, instead of a 1 if the word is present in a document, the vector can contain the number of occurences of the word.\n",
    "\n",
    "Things will be clearer with an example.\n",
    "\n",
    "Let's code Bag of words using nltk. We'll code it from scratch and will make use of the stop words removal and lemmatization we have seen previously.\n",
    "\n",
    "\n",
    "We have a corpus of 3 short documents, of one sentence each.\n",
    "\n",
    "Document 1: **_<span style=\"color:blue;\">I wish I was at home [...] with the kettle just beginning to sing!</span>_**\n",
    "\n",
    "Document 2: **_<span style=\"color:blue;\">Home is now behind you, the world is ahead!</span>_**\n",
    "\n",
    "Document 3: **_<span style=\"color:blue;\">He jumped up to [...] put his kettle on â€” and found he was not home at all.</span>_**\n",
    "\n",
    "First, let's build the vocabulary, the list of all individual words appearing in the corpus. We do so by first removing the stop words and punctuation, and then lemmatizing the remaining words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['begin', 'sing', 'put', 'wish', 'behind', 'home', 'ahead', 'jump', 'find', 'kettle', 'world']\n"
     ]
    }
   ],
   "source": [
    "t1 = \"I wish I was at home [...] with the kettle just beginning to sing!\"\n",
    "t2 = \"Home is now behind you, the world is ahead!\"\n",
    "t3 = \"He jumped up to [...] put his kettle on â€” and found he was not home at all.\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_list_words(text):\n",
    "    lem = WordNetLemmatizer()\n",
    "    stop = set(stopwords.words('english')) \n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    f =  [w.lower() for w in tokenized]\n",
    "    return [lem.lemmatize(w.lower(), pos=\"v\") for w in tokenized \\\n",
    "           if (not w.lower() in stop and w.isalnum())]\n",
    "\n",
    "\n",
    "filtered1 = get_list_words(t1)\n",
    "filtered2 = get_list_words(t2)\n",
    "filtered3 = get_list_words(t3)\n",
    "\n",
    "vocab = list(set(filtered1+filtered2+filtered3))\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the vocabulary, let's build the vector representations for all three documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]\n",
      "[0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "def get_bow_vector(vocab, document):\n",
    "    filtered = get_list_words(document)\n",
    "    v = []\n",
    "    for w in vocab:\n",
    "        c = 1 if w in filtered else 0\n",
    "        v.append(c)\n",
    "    return v\n",
    "\n",
    "\n",
    "print(get_bow_vector(vocab, t1))\n",
    "print(get_bow_vector(vocab, t2))\n",
    "print(get_bow_vector(vocab, t3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first document contains the word **_home_**, so the first element of the vector is a **1**. But it doesn't contain the word **jump**, so the second element of the vector is a **0**.\n",
    "\n",
    "## With scikit-learn\n",
    "Now, let's see how to use Bag of words with **scikit-learn**. By default, it doesn't remove the stop words and counts the occurences of the words. It's highly customizable, so this behavior can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ahead', 0), ('all', 1), ('and', 2), ('at', 3), ('beginning', 4), ('behind', 5), ('found', 6), ('he', 7), ('his', 8), ('home', 9), ('is', 10), ('jumped', 11), ('just', 12), ('kettle', 13), ('not', 14), ('now', 15), ('on', 16), ('put', 17), ('sing', 18), ('the', 19), ('to', 20), ('up', 21), ('was', 22), ('wish', 23), ('with', 24), ('world', 25), ('you', 26)]\n",
      "[[0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0]\n",
      " [1 0 0 0 0 1 0 0 0 1 2 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1]\n",
      " [0 1 1 1 0 0 1 2 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "corpus = [t1,t2,t3]\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Print vocabulary in order\n",
    "d = vectorizer.vocabulary_\n",
    "import operator\n",
    "vocab = sorted(d.items(), key=operator.itemgetter(1))\n",
    "print(vocab)\n",
    "\n",
    "# Get BoW vectors\n",
    "vector = vectorizer.transform(corpus)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TF-IDF: Term Frequencyâ€“Inverse Document Frequency\n",
    "\n",
    "TF(t,d) = Number of occurence of word _t_ in document _d_\n",
    "\n",
    "IDF(t,C) = $\\log\\frac{N}{|{d\\in C: t\\in d}|}$\n",
    "\n",
    "It's the _log_ of the number of documents in the corpus divided by the number of documents that contain the word _t_.\n",
    "An additional term +1 is often added to the denominator to prevent divisions by 0.\n",
    "\n",
    "TF-IDF score: TF(t,d)$*$IDF(t,C)\n",
    "\n",
    "Let's run TF-IDF on the same corpus of documents we used for BoW. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "[('ahead', 0), ('all', 1), ('and', 2), ('at', 3), ('beginning', 4), ('behind', 5), ('found', 6), ('he', 7), ('his', 8), ('home', 9), ('is', 10), ('jumped', 11), ('just', 12), ('kettle', 13), ('not', 14), ('now', 15), ('on', 16), ('put', 17), ('sing', 18), ('the', 19), ('to', 20), ('up', 21), ('was', 22), ('wish', 23), ('with', 24), ('world', 25), ('you', 26)]\n",
      "Idf for each word:\n",
      "[1.69314718 1.69314718 1.69314718 1.28768207 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718 1.69314718 1.         1.69314718 1.69314718\n",
      " 1.69314718 1.28768207 1.69314718 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.28768207 1.28768207 1.69314718 1.28768207 1.69314718\n",
      " 1.69314718 1.69314718 1.69314718]\n",
      "TF-IDF scores for document 1:\n",
      "[[0.         0.         0.         0.26492845 0.34834908 0.\n",
      "  0.         0.         0.         0.20574058 0.         0.\n",
      "  0.34834908 0.26492845 0.         0.         0.         0.\n",
      "  0.34834908 0.26492845 0.26492845 0.         0.26492845 0.34834908\n",
      "  0.34834908 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [t1,t2,t3]\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Print vocabulary in order\n",
    "d = vectorizer.vocabulary_\n",
    "import operator\n",
    "vocab = sorted(d.items(), key=operator.itemgetter(1))\n",
    "print('Vocabulary:')\n",
    "print(vocab)\n",
    "\n",
    "print('Idf for each word:')\n",
    "print(vectorizer.idf_)\n",
    "\n",
    "#Get Tf-Idf scores for the first document\n",
    "print('TF-IDF scores for document 1:')\n",
    "vector = vectorizer.transform([t1])\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER\n",
    "Named Entity Recognition identifies entities in a text. It's a very useful tool in NLP.\n",
    "Let's see an example using Spacy.\n",
    "\n",
    "Text example: **_<span style=\"color:blue;\">Not that Belladonna Took ever had any adventures after she became Mrs. Bungo Baggins. Bungo, that was Bilboâ€™s father, built the most luxurious hobbit-hole for her (and partly with her money) that was to be found either under The Hill or over The Hill or across The Water, and there they remained to the end of their days.</span>_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Not that \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Belladonna Took\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " ever had any adventures after she became Mrs. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bungo Baggins\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bungo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", that was \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bilbo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "â€™s father, built the most luxurious hobbit-hole for her (and partly with her money) that was to be found either under The Hill or over \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Hill\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " or across \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Water\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       ", and there they remained to \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the end of their days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "text = \"\"\"Not that Belladonna Took ever had any adventures after she became Mrs. Bungo Baggins. Bungo, that was Bilboâ€™s father, built the most luxurious hobbit-hole for her (and partly with her money) that was to be found either under The Hill or over The Hill or across The Water, and there they remained to the end of their days.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, jupyter=True, style='ent')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NER](https://raw.githubusercontent.com/gcastex/Introduction_to_NLP_workshop/master/Figures/NER.png?token=AEOBFLBOBHVE7RWEKRTZE7S642M66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
